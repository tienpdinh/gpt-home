# GPT-Home with Ollama Integration

## ‚úÖ **Now Runs Perfectly on k3s!**

The GPT-Home project has been updated to use **Ollama** running on your home PC, which removes all the resource constraints and complexity of running LLMs directly in Kubernetes.

## üèóÔ∏è **Architecture**

```
[k3s Cluster] ‚îÄ‚îÄHTTP‚îÄ‚îÄ> [Your PC: Ollama] ‚îÄ‚îÄ> [LLM Models]
     ‚îÇ
     ‚îî‚îÄ‚îÄ> [HomeAssistant]
```

- **k3s**: Lightweight deployment (256MB RAM, 0.25 CPU)
- **Your PC**: Runs Ollama with full GPU/CPU power
- **Network**: Simple HTTP API calls between k3s and Ollama

## üöÄ **Setup Instructions**

### **1. Install Ollama on Your PC**

```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama (runs on port 11434)
ollama serve

# Pull a model (in another terminal)
ollama pull llama3.2  # or llama3.2:3b for smaller model
```

### **2. Configure Ollama for Network Access**

By default, Ollama only accepts localhost connections. Configure it to accept connections from your k3s cluster:

```bash
# Set environment variable to accept external connections
export OLLAMA_HOST=0.0.0.0:11434

# Or add to your shell profile (.bashrc/.zshrc)
echo 'export OLLAMA_HOST=0.0.0.0:11434' >> ~/.bashrc

# Restart Ollama
ollama serve
```

### **3. Update k3s Configuration**

Edit `deployments/k3s/configmap.yaml`:

```yaml
data:
  ollama-url: \"http://YOUR_PC_IP:11434\"  # Replace with your PC's IP
  ollama-model: \"llama3.2\"               # Or your preferred model
```

### **4. Deploy to k3s**

```bash
# Update the configmap IP address first
kubectl apply -f deployments/k3s/configmap.yaml
kubectl apply -f deployments/k3s/pvc.yaml
kubectl apply -f deployments/k3s/deployment.yaml

# Check if it's running
kubectl get pods -l app=gpt-home
kubectl logs -l app=gpt-home
```

## üîß **Configuration Options**

### **Environment Variables**

| Variable | Default | Description |
|----------|---------|-------------|
| `OLLAMA_URL` | `http://localhost:11434` | Ollama server URL |
| `OLLAMA_MODEL` | `llama3.2` | Model name to use |
| `LLM_MAX_TOKENS` | `512` | Max response length |
| `LLM_TEMPERATURE` | `0.7` | Creativity (0.1-1.0) |
| `LLM_TIMEOUT` | `30` | Request timeout (seconds) |

### **Recommended Models**

| Model | Size | RAM Needed | Use Case |
|-------|------|------------|----------|
| `llama3.2:1b` | 1.3GB | 2GB | Fastest, basic tasks |
| `llama3.2:3b` | 2.0GB | 4GB | Good balance |
| `llama3.2` | 4.7GB | 8GB | Best quality |
| `qwen2.5:7b` | 4.4GB | 8GB | Excellent for chat |

## üéØ **Benefits of This Approach**

### **‚úÖ k3s Friendly**
- **Tiny footprint**: 256MB RAM vs 4-8GB for local LLM
- **Fast startup**: No model loading in containers
- **Simple deployment**: Standard HTTP client, no CGO
- **Easy scaling**: Can run multiple replicas if needed

### **‚úÖ Performance**
- **GPU acceleration**: Use your PC's GPU through Ollama
- **Model choice**: Easy to switch models with `ollama pull`
- **No resource limits**: Use full PC capabilities

### **‚úÖ Development**
- **Easy testing**: Run Ollama locally during development
- **Model management**: `ollama list`, `ollama rm`, etc.
- **Hot swapping**: Change models without rebuilding containers

## üß™ **Testing the Integration**

### **1. Test Ollama Directly**
```bash
# Test Ollama is working
curl http://localhost:11434/api/generate \\
  -d '{\"model\":\"llama3.2\",\"prompt\":\"Hello\",\"stream\":false}'
```

### **2. Test from k3s Pod**
```bash
# Get into the GPT-Home pod
kubectl exec -it deployment/gpt-home -- sh

# Test connection to your PC
wget -qO- http://YOUR_PC_IP:11434/api/tags
```

### **3. Test Smart Home Commands**
```bash
# Test the chat endpoint
curl -X POST http://gpt-home.local/api/v1/chat \\
  -H \"Content-Type: application/json\" \\
  -d '{\"message\":\"turn on the lights\"}'
```

## üîí **Security Considerations**

- **Network**: Ollama traffic is unencrypted HTTP within your home network
- **Firewall**: Consider restricting Ollama port 11434 to your local network
- **Models**: All processing stays within your home network (privacy-first)

## üêõ **Troubleshooting**

### **Connection Issues**
1. Check PC firewall allows port 11434
2. Verify Ollama is bound to 0.0.0.0, not 127.0.0.1
3. Test with `telnet YOUR_PC_IP 11434` from k3s node

### **Model Issues**
1. Ensure model is pulled: `ollama list`
2. Check model name matches configmap
3. Verify PC has enough RAM for the model

### **Performance Issues**
1. Use smaller models for faster responses
2. Adjust `LLM_TIMEOUT` if requests timeout
3. Consider `LLM_MAX_TOKENS` for shorter responses

## üìä **Resource Usage**

### **k3s Cluster (per pod)**
- **RAM**: 256MB (was 4-8GB)
- **CPU**: 0.25 cores (was 2+ cores)
- **Storage**: 1GB data only (was 5GB+ for models)

### **Your PC**
- **RAM**: Model-dependent (1-8GB)
- **CPU/GPU**: Full utilization available
- **Network**: ~1KB per request

---

**Result**: GPT-Home now runs perfectly on k3s with real LLM capabilities! üéâ